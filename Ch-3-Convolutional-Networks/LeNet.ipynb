{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Convolutional Networks (LeNet Architecture).\n",
    "---\n",
    "__Introduction to Convolutional Networks__ [[1]](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "\n",
    "Convolutional neural networks (CNNs) are one of the most popular class of neural networks architectures with a remarkable performance in many (and continously growing) set of computer vision tasks. The __LeNet__ architecture [[2]](http://yann.lecun.com/exdb/lenet/) (credited to Yann LeCun) is one of the basic convolutional architectures. The (modified) architecture takes advantage of few things:\n",
    "* Convolutional filters are one of __basic operations__ in computer vision that are used to perform multiple tasks such as edge detection.\n",
    "* Convolutional filters are learnt by exploiting __spatial correlations between pixels in an image__ as pixels are likely correlated to others in their locality rather than at different parts of the image.\n",
    "* Having a shared set of filters (i.e. a single filter processing the entire image), the __number of parameters in the network are reduced__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# Import the libraries and load the datasets.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LeNet in Tensorflow\n",
    "Following are the steps to implement a LeNet architecture in Tensorflow.\n",
    "The model is implemented to classify digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import MNIST data.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check previous section for details on MNIST dataset.\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 55000\n",
      "Validation Size: 5000\n",
      "Test Size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define some standard parameters.\n",
    "img_h = 28\n",
    "img_w = 28\n",
    "channels = 1\n",
    "n_classes = 10\n",
    "\n",
    "# Training, validation, testing...\n",
    "train_x = mnist[0].images\n",
    "train_y = mnist[0].labels\n",
    "print(\"Training Size: {}\".format(len(train_x)))\n",
    "\n",
    "val_x = mnist[1].images\n",
    "val_y = mnist[1].labels\n",
    "print(\"Validation Size: {}\".format(len(val_x)))\n",
    "\n",
    "test_x = mnist[2].images\n",
    "test_y = mnist[2].labels\n",
    "print(\"Test Size: {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__: Create placeholders for the input images and output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer size.\n",
    "layer_size_1 = 32\n",
    "layer_size_2 = 32\n",
    "\n",
    "# NOTE: The name of the variable is optional.\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name=\"Y\")\n",
    "lr_rate = tf.placeholder(tf.float32, shape=(), name=\"lr\")\n",
    "input_layer = tf.reshape(x, [-1, img_h, img_w, channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convolution Operation.__\n",
    "\n",
    "$C(I,f) = \\sum_H{\\sum_W{\\sum_{(hxw)}f[j].X[j]}}$ \n",
    "\n",
    "where __I__ is the image of size __(H, W)__, __f__ is a filter (weights) of size __(h, w)__.\n",
    "\n",
    "__Step 2__: Once the placeholders have been created, add convolutional layers. For adding a 2D convolutional layer, we use `tf.layers.conv2d` function. Convolution layers in different libraries take the input in the form of 4D Tensor consisting of batch_size, filter height, filter width, channel. But the order of the dimensions differ.\n",
    "\n",
    "__It is important to check the order of the dimensions depending on the library used.__\n",
    "\n",
    "In case of tensorflow, the order is `batch_size, filter height, filter width, channel`.\n",
    "\n",
    "__What is a channel and why is it present?__\n",
    "\n",
    "Think about a standard RGB image. The image is represented as a 3D Tensor of size $[3, 255, 255]$ each __channel__ matrix of 255x255 representing R, G & B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/layers\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(inputs=input_layer,\n",
    "                         filters=32,\n",
    "                         kernel_size=[5, 5],\n",
    "                         padding=\"same\",\n",
    "                         activation=tf.nn.relu)\n",
    "# Pooling layer #1\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                         filters=32,\n",
    "                         kernel_size=[5, 5],\n",
    "                         padding=\"same\",\n",
    "                         activation=tf.nn.relu)\n",
    "# Pooling layer #2\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What does the pooling layer do?__\n",
    "\n",
    "The pooling layer broadly achieves two important things:\n",
    "* A part of the image covered by the pooling layer size is represented by the maximum value of that patch. This is way learn an abstract representation of the patch.\n",
    "* It reduces the size of the image on which the next layer has to convolve.\n",
    "\n",
    "Once the convolutional layers have been added to the network, the standard __fully connected or dense__ layer is added to network. But there is problem that needs to be resolved. Dense layers accept vectors of size 1xN where N is the number of rows in Dense layer. But the output of the convolutional layer (even after pooling) is 2D. Hence like the original image [28x28] was __flattened__ to [1x784], the output of pool2 is __flattened__ to [1x7 \\* 7 \\* 64].\n",
    "\n",
    "__Try: Calculate how the output size is [1x7 \\* 7 \\* 64]__\n",
    "\n",
    "__Step 3__: Add the Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight & bias.\n",
    "# Hidden layer.\n",
    "w_1 = tf.get_variable(shape=[7 * 7 * 32, layer_size_1], name=\"w_1\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "b_1 = tf.get_variable(shape=[layer_size_1], name=\"b_1\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "\n",
    "# Output layer.\n",
    "w_o = tf.get_variable(shape=[layer_size_1, 10], name=\"w_o\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "b_o = tf.get_variable(shape=[10], name=\"b_o\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "\n",
    "# NOTE: Initializations are important.\n",
    "# Zero initialization: initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: Perform the rest of the operations similar to a Feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted Y.\n",
    "h_1 = tf.nn.relu(tf.add(tf.matmul(pool2_flat, w_1), b_1)) # <--- Add ReLU activation.\n",
    "# h_1 = tf.sigmoid(tf.add(tf.matmul(pool2_flat, w_1), b_1)) # <--- Add Sigmoid activation.\n",
    "y_pred = tf.nn.softmax(tf.add(tf.matmul(h_1, w_o), b_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: Once the predicted $y$ has been computed, define the loss between the predicted $y$ and the actual $y$.\n",
    "\n",
    "With logistic regression, the loss function is __categorical cross entropy__.\n",
    "\n",
    "\n",
    "_Cross Entropy Loss_: $H(p, q) = -\\sum_xp(x)log(q(x))$\n",
    "\n",
    "__Try__: Calculate $H(p, q)$ for a binary classification $(0, 1)$.\n",
    "\n",
    "Important note: [NaN Bug](https://stackoverflow.com/questions/33712178/tensorflow-nan-bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf.multiply(y, tf.log(y_pred)), axis=1))\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf.multiply(y,\n",
    "                                                          tf.log(tf.clip_by_value(y_pred,\n",
    "                                                                                  1e-10,1.0))),\n",
    "                                                          axis=1))\n",
    "\n",
    "# The tensorflow function available.\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,\n",
    "#                                                                        logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: The loss shows how far we are from the actual $y$ value. Use the loss to change the weights by calulating the gradient w.r.t $w$. We use a stochastic gradient descent optimizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient descent optimizer with the set learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr_rate)\n",
    "\n",
    "# Run the optimizer to minimize loss\n",
    "# Tensorflow automatically computes the gradients for the loss function!!!\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Gradient Clipping.\n",
    "# gvs = optimizer.compute_gradients(cross_entropy)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# train = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5__: Add summaries for the variables that are to be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function.\n",
    "# https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            \n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            \n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "# Define summaries.\n",
    "variable_summaries(w_1, \"weights\")\n",
    "variable_summaries(b_1, \"bias\")\n",
    "variable_summaries(cross_entropy, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6__: `train` the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 7__: Compute the accuracy.\n",
    "\n",
    "`tf.argmax` returns the largest value along a specific axis of the vector (in this case 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the correct prediction by taking the maximum value from the prediction class\n",
    "# and checking it with the actual class. The result is a boolean column vector\n",
    "correct_predictions = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 8__: With the histogram being generated for each variable. `merge_all` the summaries.\n",
    "The logs are written to the `logs/logistic/tf/` which is the logs sub-directory from the current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5. Batch #: 900/1000. Loss: 0.05. Train Accuracy: 0.98\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Define some hyper-parameters.\n",
    "lr = 0.01\n",
    "epochs = 5\n",
    "batch_size = 55\n",
    "log_dir = 'logs/lenet/tf/' # Tensorboard log directory.\n",
    "batch_limit = 100\n",
    "\n",
    "# Train the model.\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Create the writer.\n",
    "    # Merge all the summaries and write them.\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    num_batches = int(len(train_x)/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            y_p, curr_w, curr_b,\\\n",
    "            curr_loss, _, summary, cur_acc = sess.run([y_pred, w_1, b_1, cross_entropy,\n",
    "                                                      train, merged, accuracy],\n",
    "                                                      feed_dict = {x: batch_xs,\n",
    "                                                                   y: batch_ys,\n",
    "                                                                   lr_rate: lr})\n",
    "            if batch_num % batch_limit == 0:\n",
    "                # IMP: Add the summary for each epoch.\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                # Print the loss\n",
    "                print(\"Epoch: %d/%d. Batch #: %d/%d. Loss: %.2f. Train Accuracy: %.2f\"\n",
    "                      %(epoch+1, epochs, batch_num, num_batches, curr_loss, cur_acc))\n",
    "    \n",
    "    # Testing.\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x: test_x,                                                   y: test_y})\n",
    "    print(\"Test Accuracy: %.2f\"%test_accuracy)\n",
    "    train_writer.close() # <-------Important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try: 1. Use a sigmoid activation instead of ReLU.__\n",
    "\n",
    "__Try: 2. Add another convolution layer (of size 5x5), maxpooling (2x2) and check the output.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Keras Implementation.\n",
    "Similar to the example in linear regression, Keras makes it __easy__ to generate summaries so that it can be visualized in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Conv2D, Reshape, MaxPooling2D, Flatten\n",
    "from keras.initializers import random_normal\n",
    "from keras.models import Model\n",
    "from keras import optimizers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorboard add it from __keras backend__. `keras.callbacks.TensorBoard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Activations](https://keras.io/activations/) - A list of all the activations that are present in Keras. Using the Function API rather than the Sequential Model. Output of every layer is a __Keras Tensor__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer to take an input.\n",
    "input_l = Input(shape=np.array([784]))\n",
    "input_r = Reshape((28, 28, 1))(input_l)\n",
    "\n",
    "# Add convolutional layer-1.\n",
    "conv1 = Conv2D(32, (5, 5), padding='same', activation='relu')(input_r)\n",
    "max1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "# Add convolutional layer-2.\n",
    "conv2 = Conv2D(32, (5, 5), padding='same', activation='relu')(max1)\n",
    "max1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flat = Flatten()(max1)\n",
    "\n",
    "# Compute Wx + b.\n",
    "dense_1 = Dense(layer_size_1, activation='relu')(flat) # <-- Thats it!\n",
    "output = Dense(10, activation='softmax')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                50208     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 77,002\n",
      "Trainable params: 77,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model and compile it.\n",
    "model = Model(inputs=[input_l], outputs=[output])\n",
    "model.summary() # Get the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.sgd(lr=lr)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# NOTE: Add Tensorboard after compiling.\n",
    "tensorboard = TensorBoard(log_dir=\"logs/lenet/keras/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's pretty much it!__\n",
    "Add `callbacks=[tensorboard]` to the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3effa5c73b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "# Add a callback.\n",
    "model.fit(x=train_x, y=train_y, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=0, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Predict the y's.\n",
    "y_p = model.predict(test_x)\n",
    "y_p_loss = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: ['loss', 'acc']\n",
      "Loss: 0.0731152933508, Accuracy: 0.9776\n"
     ]
    }
   ],
   "source": [
    "# Plot them.\n",
    "print(\"Evaluation Metrics: \" + str(model.metrics_names))\n",
    "print(\"Loss: {}, Accuracy: {}\".format(y_p_loss[0], y_p_loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's an example with TensorBoard!__\n",
    "\n",
    "Tensorboard command: `$> tensorboard --logdir <log directory>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
