{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multilayer Perceptron.\n",
    "---\n",
    "__Introduction to MLPs__ [[1]](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "Multilayer Perceptron's or Feed forward Networks are _deeper_ version of a logistic regression model (for classification tasks). In case of logistic regression, $w\\bullet x + b$ was implemented before converting it to a probability distribution (with softmax). With MLPs, we add __hidden layers__ i.e. multiple layers of $w\\bullet x + b$ to learn __richer & abstract__ representations of the images.\n",
    "\n",
    "This method though comes with a problem. If the we were to combine a series of linear functions, the output before softmax ultimately will be linear. Hence MLPs add a non-linear function called an __activation function__ [[2]](https://en.wikipedia.org/wiki/Activation_function) to create a __non-linear mapping__ between the input and the output.\n",
    "\n",
    "There are multiple activation functions from __Rectified Linear Units (ReLU), sigmoid (old school), tanh, Leaky ReLU__ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# Import the libraries and load the datasets.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feed Forward Networks in Tensorflow\n",
    "Following are the steps to implement a simple MLP regression function in Tensorflow.\n",
    "The model is implemented to classify digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import MNIST data.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check previous section for details on MNIST dataset.\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 55000\n",
      "Validation Size: 5000\n",
      "Test Size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define some standard parameters.\n",
    "img_h = 28\n",
    "img_w = 28\n",
    "n_classes = 10\n",
    "\n",
    "# Training, validation, testing...\n",
    "train_x = mnist[0].images\n",
    "train_y = mnist[0].labels\n",
    "print(\"Training Size: {}\".format(len(train_x)))\n",
    "\n",
    "val_x = mnist[1].images\n",
    "val_y = mnist[1].labels\n",
    "print(\"Validation Size: {}\".format(len(val_x)))\n",
    "\n",
    "test_x = mnist[2].images\n",
    "test_y = mnist[2].labels\n",
    "print(\"Test Size: {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__: Like linear regression, define the input $x$, output $y$ and weight $w$ and bias $b$. Each MNIST image is of size $(28, 28)$. This is image is _squashed_ into a vector of size $(1$x$784)$. n_classes represents the total number of digits $(10)$.\n",
    "\n",
    "__NOTE: Common layer sizes for the hidden layers tend to be of the power of 2 (16, 32, 64 .. 256)__. It is also common these days to keeps the size of the hidden layers constant and go _deeper_ rather _wider_. This is based on the theory that neural networks learns features higher and higher abstract form as the network grows deeper (This though is more empirical with some theory to back it).\n",
    "\n",
    "While initializing weight vectors `w_1`, `b_1`, `w_o` & `b_o` avoid initializing with zero values. This is because the outputs of all the activations will be 0. The gradients generated for each neuron remain the same. Use random initializations where the weights values are sampled from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer size.\n",
    "layer_size_1 = 128\n",
    "layer_size_2 = 128\n",
    "\n",
    "# NOTE: The name of the variable is optional.\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name=\"Y\")\n",
    "lr_rate = tf.placeholder(tf.float32, shape=(), name=\"lr\")\n",
    "\n",
    "# Weight & bias.\n",
    "# Hidden layer.\n",
    "w_1 = tf.get_variable(shape=[784, layer_size_1], name=\"w_1\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "b_1 = tf.get_variable(shape=[layer_size_1], name=\"b_1\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "\n",
    "# Output layer.\n",
    "w_o = tf.get_variable(shape=[layer_size_1, 10], name=\"w_o\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "b_o = tf.get_variable(shape=[10], name=\"b_o\",\n",
    "                      initializer=tf.random_normal_initializer())\n",
    "\n",
    "# NOTE: Initializations are important.\n",
    "# Zero initialization: initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: Once the placeholders & variable have been created, compute the $y$. The softmax function is a generalized form of the logistic function.\n",
    "\n",
    "$P(y = j| x) = \\frac{e^{x^Tw_j}}{\\sum_{k=1}^{K}e^{x^Tw_k}}$\n",
    "\n",
    "__Activation Function: One of the most popular activation function (especially in computer vision) is Rectified Linear Unit (ReLU).__\n",
    "\n",
    "$R(x) = \\begin{cases} \n",
    "          x & x\\geq 0 \\\\\n",
    "          0 & x\\lt 0 \n",
    "       \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted Y.\n",
    "# h_1 = tf.nn.relu(tf.add(tf.matmul(x, w_1), b_1)) # <--- Add ReLU activation.\n",
    "h_1 = tf.sigmoid(tf.add(tf.matmul(x, w_1), b_1)) # <--- Add Sigmoid activation.\n",
    "y_pred = tf.nn.softmax(tf.add(tf.matmul(h_1, w_o), b_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: Once the predicted $y$ has been computed, define the loss between the predicted $y$ and the actual $y$.\n",
    "\n",
    "With logistic regression, the loss function is __categorical cross entropy__.\n",
    "\n",
    "\n",
    "_Cross Entropy Loss_: $H(p, q) = -\\sum_xp(x)log(q(x))$\n",
    "\n",
    "__Try__: Calculate $H(p, q)$ for a binary classification $(0, 1)$.\n",
    "\n",
    "Important note: [NaN Bug](https://stackoverflow.com/questions/33712178/tensorflow-nan-bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf.multiply(y, tf.log(y_pred)), axis=1))\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf.multiply(y,\n",
    "#                                                           tf.log(tf.clip_by_value(y_pred,\n",
    "#                                                                                   1e-10,1.0))),\n",
    "#                                                           axis=1))\n",
    "\n",
    "# The tensorflow function available.\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,\n",
    "#                                                                        logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: The loss shows how far we are from the actual $y$ value. Use the loss to change the weights by calulating the gradient w.r.t $w$. We use a stochastic gradient descent optimizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient descent optimizer with the set learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr_rate)\n",
    "\n",
    "# Run the optimizer to minimize loss\n",
    "# Tensorflow automatically computes the gradients for the loss function!!!\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Gradient Clipping.\n",
    "# gvs = optimizer.compute_gradients(cross_entropy)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# train = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5__: Add summaries for the variables that are to be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function.\n",
    "# https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            \n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            \n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "# Define summaries.\n",
    "variable_summaries(w_1, \"weights\")\n",
    "variable_summaries(b_1, \"bias\")\n",
    "variable_summaries(cross_entropy, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6__: `train` the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 7__: Compute the accuracy.\n",
    "\n",
    "`tf.argmax` returns the largest value along a specific axis of the vector (in this case 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the correct prediction by taking the maximum value from the prediction class\n",
    "# and checking it with the actual class. The result is a boolean column vector\n",
    "correct_predictions = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 8__: With the histogram being generated for each variable. `merge_all` the summaries.\n",
    "The logs are written to the `logs/logistic/tf/` which is the logs sub-directory from the current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5. Batch #: 900/1000. Loss: 0.79. Train Accuracy: 0.78\n",
      "Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Define some hyper-parameters.\n",
    "lr = 0.01\n",
    "epochs = 5\n",
    "batch_size = 55\n",
    "log_dir = 'logs/mlp/tf/' # Tensorboard log directory.\n",
    "batch_limit = 100\n",
    "\n",
    "# Train the model.\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Create the writer.\n",
    "    # Merge all the summaries and write them.\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    num_batches = int(len(train_x)/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            y_p, curr_w, curr_b,\\\n",
    "            curr_loss, _, summary, cur_acc = sess.run([y_pred, w_1, b_1, cross_entropy,\n",
    "                                                      train, merged, accuracy],\n",
    "                                                      feed_dict = {x: batch_xs,\n",
    "                                                                   y: batch_ys,\n",
    "                                                                   lr_rate: lr})\n",
    "            if batch_num % batch_limit == 0:\n",
    "                # IMP: Add the summary for each epoch.\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                # Print the loss\n",
    "                print(\"Epoch: %d/%d. Batch #: %d/%d. Loss: %.2f. Train Accuracy: %.2f\"\n",
    "                      %(epoch+1, epochs, batch_num, num_batches, curr_loss, cur_acc))\n",
    "    \n",
    "    # Testing.\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={x: test_x,                                                   y: test_y})\n",
    "    print(\"Test Accuracy: %.2f\"%test_accuracy)\n",
    "    train_writer.close() # <-------Important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try: 1. Make all the weights zero and test it.__\n",
    "\n",
    "__Try: 2. Use a ReLU activation instead of sigmoid.__\n",
    "\n",
    "__Try: 3. Add another hidden layer and check the output.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Keras Implementation.\n",
    "Similar to the example in linear regression, Keras makes it __easy__ to generate summaries so that it can be visualized in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import random_normal\n",
    "from keras.models import Model\n",
    "from keras import optimizers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorboard add it from __keras backend__. `keras.callbacks.TensorBoard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Activations](https://keras.io/activations/) - A list of all the activations that are present in Keras. Using the Function API rather than the Sequential Model. Output of every layer is a __Keras Tensor__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer to take an input.\n",
    "input_l = Input(shape=np.array([784]))\n",
    "# Compute Wx + b.\n",
    "dense_1 = Dense(layer_size_1, activation='sigmoid')(input_l) # <-- Thats it!\n",
    "output = Dense(10, activation='softmax')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model and compile it.\n",
    "model = Model(inputs=[input_l], outputs=[output])\n",
    "model.summary() # Get the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.sgd(lr=lr)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# NOTE: Add Tensorboard after compiling.\n",
    "tensorboard = TensorBoard(log_dir=\"logs/logistic/keras/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's pretty much it!__\n",
    "Add `callbacks=[tensorboard]` to the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3effa9b69f50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "# Add a callback.\n",
    "model.fit(x=train_x, y=train_y, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=0, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9760/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Predict the y's.\n",
    "y_p = model.predict(test_x)\n",
    "y_p_loss = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: ['loss', 'acc']\n",
      "Loss: 0.481021496654, Accuracy: 0.8847\n"
     ]
    }
   ],
   "source": [
    "# Plot them.\n",
    "print(\"Evaluation Metrics: \" + str(model.metrics_names))\n",
    "print(\"Loss: {}, Accuracy: {}\".format(y_p_loss[0], y_p_loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's an example with TensorBoard!__\n",
    "\n",
    "Tensorboard command: `$> tensorboard --logdir <log directory>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
