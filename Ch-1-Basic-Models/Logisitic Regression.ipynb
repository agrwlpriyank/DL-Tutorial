{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Logistic Regression.\n",
    "---\n",
    "__Introduction to Logistic Regression__ [[1]](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "Logistic regression is a standard machine learning model where the dependent variable is __categorical__ (in regression, the variable that is predicted is a __continuous variable__ while in classification tasks it is __categorical__).\n",
    "\n",
    "__Logistic Function__: $f(x) = \\frac{1}{1 + e^{-x}}$ where $x$ is the input and $f(x)$ is the dependent variable. The function can be interpreted as a probability distribution as its values are between $0$ & $1$.\n",
    "\n",
    "Regression Function: $f(x) = \\frac{1}{1 + e^-{(w \\bullet x + b)}}$\n",
    "\n",
    "Logistic regression gives a probability distribution over the values of the dependent variable and the predict is the value that has the __maximum likelihood__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# Import the libraries and load the datasets.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Logistic Regression in Tensorflow\n",
    "Following are the steps to implement a simple logistic regression function in Tensorflow.\n",
    "The model is implemented to classify digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import MNIST data.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check previous section for details on MNIST dataset.\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 55000\n",
      "Validation Size: 5000\n",
      "Test Size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Define some standard parameters.\n",
    "img_h = 28\n",
    "img_w = 28\n",
    "n_classes = 10\n",
    "\n",
    "# Training, validation, testing...\n",
    "train_x = mnist[0].images\n",
    "train_y = mnist[0].labels\n",
    "print(\"Training Size: {}\".format(len(train_x)))\n",
    "\n",
    "val_x = mnist[1].images\n",
    "val_y = mnist[1].labels\n",
    "print(\"Validation Size: {}\".format(len(val_x)))\n",
    "\n",
    "test_x = mnist[2].images\n",
    "test_y = mnist[2].labels\n",
    "print(\"Test Size: {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__: Like linear regression, define the input $x$, output $y$ and weight $w$ and bias $b$. Each MNIST image is of size $(28, 28)$. This is image is _squashed_ into a vector of size $(1$x$784)$. n_classes represents the total number of digits $(10)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The name of the variable is optional.\n",
    "x = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10), name=\"Y\")\n",
    "lr_rate = tf.placeholder(tf.float32, shape=(), name=\"lr\")\n",
    "\n",
    "# Weight & bias.\n",
    "w = tf.get_variable(shape=[784, 10], name=\"w\", initializer=tf.zeros_initializer())\n",
    "b = tf.get_variable(shape=[10], name=\"b\", initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: Once the placeholders & variable have been created, compute the $y$. The softmax function is a generalized form of the logistic function.\n",
    "\n",
    "$P(y = j| x) = \\frac{e^{x^Tw_j}}{\\sum_{k=1}^{K}e^{x^Tw_k}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted Y.\n",
    "y_pred = tf.nn.softmax(tf.add(tf.matmul(x, w), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3__: Once the predicted $y$ has been computed, define the loss between the predicted $y$ and the actual $y$.\n",
    "\n",
    "With logistic regression, the loss function is __categorical cross entropy__.\n",
    "\n",
    "\n",
    "_Cross Entropy Loss_: $H(p, q) = -\\sum_xp(x)log(q(x))$\n",
    "\n",
    "__Try__: Calculate $H(p, q)$ for a binary classification $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-1.0*(y*tf.log(y_pred)))\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=1))\n",
    "\n",
    "# The tensorflow function available.\n",
    "# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__: The loss shows how far we are from the actual $y$ value. Use the loss to change the weights by calulating the gradient w.r.t $w$. We use a stochastic gradient descent optimizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient descent optimizer with the set learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr_rate)\n",
    "\n",
    "# Run the optimizer to minimize loss\n",
    "# Tensorflow automatically computes the gradients for the loss function!!!\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5__: Add summaries for the variables that are to be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function.\n",
    "# https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            \n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            \n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "    \n",
    "# Define summaries.\n",
    "variable_summaries(w, \"weights\")\n",
    "variable_summaries(b, \"bias\")\n",
    "variable_summaries(cross_entropy, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6__: `train` the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 7__: Compute the accuracy.\n",
    "\n",
    "`tf.argmax` returns the largest value along a specific axis of the vector (in this case 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the correct prediction by taking the maximum value from the prediction class\n",
    "# and checking it with the actual class. The result is a boolean column vector\n",
    "correct_predictions = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 8__: With the histogram being generated for each variable. `merge_all` the summaries.\n",
    "The logs are written to the `logs/logistic/tf/` which is the logs sub-directory from the current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15. Batch #: 900/1000. Current loss: 0.17281. Train Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Define some hyper-parameters.\n",
    "lr = 0.0005\n",
    "epochs = 15\n",
    "batch_size = 55\n",
    "log_dir = 'logs/logistic/tf/' # Tensorboard log directory.\n",
    "batch_limit = 100\n",
    "\n",
    "# Train the model.\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Create the writer.\n",
    "    # Merge all the summaries and write them.\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    num_batches = int(len(train_x)/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            y_p, curr_w, curr_b,\\\n",
    "            curr_loss, _, summary, cur_acc = sess.run([y_pred, w, b, cross_entropy,\n",
    "                                                      train, merged, accuracy],\n",
    "                                                      feed_dict = {x: batch_xs,\n",
    "                                                                   y: batch_ys,\n",
    "                                                                   lr_rate: lr})\n",
    "            if batch_num % batch_limit == 0:\n",
    "                # IMP: Add the summary for each epoch.\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.1)\n",
    "                # Print the loss\n",
    "                print(\"Epoch: %d/%d. Batch #: %d/%d. Current loss: %.5f. Train Accuracy: %.2f\"\n",
    "                      %(epoch+1, epochs, batch_num, num_batches, curr_loss, cur_acc))\n",
    "    \n",
    "    train_writer.close() # <-------Important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Keras Implementation.\n",
    "Similar to the example in linear regression, Keras makes it __easy__ to generate summaries so that it can be visualized in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import random_normal\n",
    "from keras.models import Model\n",
    "from keras import optimizers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorboard add it from __keras backend__. `keras.callbacks.TensorBoard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer to take an input.\n",
    "input_l = Input(shape=np.array([784]))\n",
    "# Compute Wx + b.\n",
    "dense = Dense(np.array([10]), activation='softmax')\n",
    "output = dense(input_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, array([10]))       7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model and compile it.\n",
    "model = Model(inputs=[input_l], outputs=[output])\n",
    "model.summary() # Get the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.sgd(lr=lr)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# NOTE: Add Tensorboard after compiling.\n",
    "tensorboard = TensorBoard(log_dir=\"logs/logistic/keras/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's pretty much it!__\n",
    "Add `callbacks=[tensorboard]` to the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3effac7d7c90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "# Add a callback.\n",
    "model.fit(x=train_x, y=train_y, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=0, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9376/10000 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Predict the y's.\n",
    "y_p = model.predict(test_x)\n",
    "y_p_loss = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: ['loss', 'acc']\n",
      "Loss: 0.652655434513, Accuracy: 0.8586\n"
     ]
    }
   ],
   "source": [
    "# Plot them.\n",
    "print(\"Evaluation Metrics: \" + str(model.metrics_names))\n",
    "print(\"Loss: {}, Accuracy: {}\".format(y_p_loss[0], y_p_loss[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's an example with TensorBoard!__\n",
    "\n",
    "Tensorboard command: `$> tensorboard --logdir <log directory>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
